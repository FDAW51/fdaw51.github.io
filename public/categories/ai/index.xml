<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on Eugene</title>
        <link>http://localhost:1313/categories/ai/</link>
        <description>Recent content in AI on Eugene</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 04 Jul 2025 21:04:14 +0800</lastBuildDate><atom:link href="http://localhost:1313/categories/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>算法导论｜chapter1</title>
        <link>http://localhost:1313/2025/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAchapter1/</link>
        <pubDate>Fri, 04 Jul 2025 21:04:14 +0800</pubDate>
        
        <guid>http://localhost:1313/2025/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAchapter1/</guid>
        <description>&lt;h1 id=&#34;算法在计算中的作用&#34;&gt;算法在计算中的作用
&lt;/h1&gt;
&lt;h2 id=&#34;算法&#34;&gt;算法：
&lt;/h2&gt;
&lt;p&gt;首先，非形式地定义一下算法：&lt;strong&gt;算法&lt;/strong&gt;是任何良定义的计算过程，该过程有输入有输出，但是局部来看，一个算法会包含多个输入输出的环节，即有多个计算步骤。&lt;/p&gt;
&lt;p&gt;比如说排序问题，对于这个问题，我们有相应的算法去解决，但是前提是我们应该先弄清楚这个问题的输入和输出是什么？才能更好地设计这个解决方案，输入是一个包含n个数的序列 $&amp;lt;a_1,a_2,\dots,a_n&amp;gt;$,而输出是一个排好序的序列 $&amp;lt;A_1,A_2,\dots,A_n&amp;gt;$ （其中 $A_1\leq A_2 \leq \dots \leq A_n$）,例如若输入$&amp;lt;31，23，45，632，123&amp;gt;$，则需要输出&amp;lt;23，31，45，123，632&amp;gt;。要实现这个算法，我们有许多种方法：插入排序，分冶法等，每一种算法中都包含对于输入序列的不同的操作，最终达到排序的效果。&lt;/p&gt;
&lt;h3 id=&#34;算法解决哪一类的问题&#34;&gt;算法解决哪一类的问题：
&lt;/h3&gt;
&lt;p&gt;算法不只是解决排序这一种问题，算法的实际应用无处不在。比如：1.互联网使得全世界的人都能快速地访问与检索大量的信息，这就需要各个网站能够管理和处理这些海量的数据，所以必须使用相应的算法来对数据进行检索和传输。2. 给定两个有序的符号序列，求出两个序列的最长公共子序列，3. 给定平面上n个点，找出这些点的凸壳（凸壳就是包含这些点的最小凸多边形），等等。&lt;/p&gt;
&lt;p&gt;虽然算法能解决的问题还有许多，但是解决这些问题的算法却展示了许多有趣算法的共同的两个特征：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于一个问题，有多个算法可以解决或者很多候选解（并未解决这个问题，只解决了一部分），但是找一个真正的解或最好的解却是一个很大的挑战。&lt;/li&gt;
&lt;li&gt;在实际生活中存在应用：比如最短路径问题的解法能够解决运输公司如何在公路或者铁路网络找到最短路径的问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;作为一种技术的算法&#34;&gt;作为一种技术的算法：
&lt;/h2&gt;
&lt;p&gt;如果计算机无限快并且计算资源（能源，芯片，储存器）是免费的，那么还有研究算法的必要吗？答案是否定的，但是这种情况目前看来是不可能实现的。那么我们就需要权衡现有的计算资源和速度，对应一个具体问题，找到一个效率最好的算法很重要。&lt;/p&gt;
&lt;p&gt;对于同一个问题，不同的算法在效率上的区别可以很大。&lt;/p&gt;
&lt;p&gt;例如，排序算法中的两个算法，插入排序和归并排序，前面的算法所花的时间大概是 $c_1n^2$,其中c1是一个不依赖于n的参数，n指的是输入的规模 （表示一个数的二进制串的长度），这个时间表明该算法所花的时间大致与n的平方成正比；第二个算法所花的时间大概是 $c_2n\lg{n}$。插入排序的因子c1通常比归并排序的因子小，因此在n很小的时候，插入排序比归并排序更快，但是当n很大时，归并排序的优势就显现出来了。&lt;/p&gt;
&lt;p&gt;可以具体一点：我们让运行插入排序的一台较快的计算机A与一台运行归并排序的一台较慢的计算机B竞争，两者都要排序一个具有1000万个数的数组（1000万个数如果都是8字节的整数，输入将占用80MB),假设A每秒可以执行10^8条指令，假设插入排序的c1为2，那么运行插入排序对于1000万个数的数组排序所需要的指令数是 $2*(10^7)^2$，那么说需要的时间就是20000秒（大约5.5个小时）&lt;/p&gt;
&lt;p&gt;但是若计算机B每秒只能执行1000万条指令，对于归并算法而言（设置c2为50），需要50nlgn条指令，将指令数除以速度得到时间约为1163秒（少于20min）&lt;/p&gt;
&lt;p&gt;因此可以看出，随着规模的增大，不同效率的算法的优势就会更加明显，因此提前设计一个兼容性好（在问题规模扩大之后，仍然能够运行）的算法也是至关重要的。&lt;/p&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;
&lt;h3 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h3&gt;
&lt;h3 id=&#34;版权信息&#34;&gt;版权信息
&lt;/h3&gt;
&lt;p&gt;本文原载于 &lt;a class=&#34;link&#34; href=&#34;https://quantum51.top&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;quantum51.top&lt;/a&gt;，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI|Why Machines Learn?</title>
        <link>http://localhost:1313/2025/aiwhy-machines-learn/</link>
        <pubDate>Sat, 19 Apr 2025 09:34:28 +0800</pubDate>
        
        <guid>http://localhost:1313/2025/aiwhy-machines-learn/</guid>
        <description>&lt;h1 id=&#34;why-machines-learn&#34;&gt;Why machines learn？
&lt;/h1&gt;
&lt;h2 id=&#34;chapter1deseperatly-seeking-pattern&#34;&gt;Chapter1:Deseperatly seeking pattern
&lt;/h2&gt;
&lt;h3 id=&#34;从现象出发&#34;&gt;从现象出发
&lt;/h3&gt;
&lt;p&gt;这里面的pattern其实就是数据中某种模式特征，打个比方，每一个人对于每一年每一个季度的天气的理解和判断，都是基于一段时间（几年）的生活，通过观察每一年的天气（数据），然后得出一些经验（特征）。&lt;/p&gt;
&lt;p&gt;然而令人惊讶的是，小小的鸭苗在没有父母的帮助下，也可以从运动的物体中找到一定的规律，这些规律可以是相似之处，也可以是不同之处，比如，小鸭苗如果看见有5个黑鸭苗和2个白鸭苗的队伍，它会知道自己是黑的还是白的，并且知道黑色和白色的差别，最后他会加入到其中一个队伍里面。&lt;/p&gt;
&lt;p&gt;这就是令人惊讶的动物的学习能力。在早些年，就有科学家提出“动物（人）是怎么学习的”这一问题，他们就想到先从学习数据中的特征入手，于是就有科学家开发了Perceptron（感知机）来模拟人类的思考。当你给这个感知机下面这些数据的时候：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;$x_1$&lt;/th&gt;
          &lt;th&gt;$x_2$&lt;/th&gt;
          &lt;th&gt;$y$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;8&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们只需要通过一点点的观察和心算就能够发现这组数据中存在的关系是：
$$
y = x_1+2x_2
$$&lt;/p&gt;
&lt;p&gt;而对于现在的机器而言无非就是一个Regression算法，这个算法的意思是：你给他很多的train data（有input和output），然后它会通过学习这些数据来给出这些量之间的线性关系（$y =w_1x_1+w_2x_2+b$），也就是学习得到 $w_1,w_2$(系数，weight)以及偏差（截距）。&lt;/p&gt;
&lt;p&gt;然后你可以通过一些测试数据来判读这一组系数的好坏（离最优的系数差多远，最优的系数当然是通过每一个数据点，但是实际上并不能很好的做到），最后得到这组数据的最优解，接下来你就可以通过这样的关系来predict不同的input，会有怎么样的output了。就好像天气一样，你可以通过之前每一天的数据来预测之后的数据一样，只不过天气与很多因素有关。&lt;/p&gt;
&lt;p&gt;所以Regression Method的具体步骤是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;给他一定的训练数据，并且指定一开始的 $w_1,w2，b$&lt;/li&gt;
&lt;li&gt;然后计算这一组的系数所给出的output $y_{predict}和y_{train}$之间的差距，然后通过这个差距反过来调节系数&lt;/li&gt;
&lt;li&gt;不断的执行，直到这个差距小到我们的要求为止&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;神经元的逻辑化&#34;&gt;神经元的逻辑化
&lt;/h3&gt;
&lt;p&gt;言归正传（Regression后面还会系统的说明），我们也许可以通过理解机器是怎么学习的来完全理解人类是怎么学习的（这里我的理解是我们可以在让机器逐步学会学习的过程中，不断地深化我们对于自己学习过程的理解）。&lt;/p&gt;
&lt;p&gt;19世纪，图灵等科学家就认为logic和computation之间有很深厚的联系，他们断言“所有的计算都可以被简化为某种逻辑”。然后就引出了这样一个问题：既然人脑是可以执行计算的，那么它是怎么样执行逻辑操作的（它底层是否像逻辑门一样呢？）。&lt;/p&gt;
&lt;p&gt;带着这样的问题，有生物学家通过类比一个神经元：&lt;img src=&#34;http://localhost:1313/2025/aiwhy-machines-learn/1.png&#34;
	width=&#34;948&#34;
	height=&#34;421&#34;
	srcset=&#34;http://localhost:1313/2025/aiwhy-machines-learn/1_hu_4cf4f3e353866372.png 480w, http://localhost:1313/2025/aiwhy-machines-learn/1_hu_f54ef82382b8a7c7.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;540px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;图中Dendrites就是神经元的树突，它负责接受各种刺激，树突中间的就是细胞体，他可以处理树突接受到的刺激（相当于进行计算），然后Axon（轴突）负责转递细胞体的结果到Axon terminals（端粒），端粒在将这个结果传递给周边其他的神经元。&lt;/p&gt;
&lt;p&gt;然后生物学家希望把这一机构转化为一个简单的计算模型，理由是：他很像一个机器，你给他一个输入（刺激），他就会给输出。因此他们想要通过类比用神经元来构建逻辑AND，OR操作。&lt;/p&gt;
&lt;p&gt;他们首先将这个神经元定义为这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/aiwhy-machines-learn/2.png&#34;
	width=&#34;974&#34;
	height=&#34;382&#34;
	srcset=&#34;http://localhost:1313/2025/aiwhy-machines-learn/2_hu_5ca8c49f76992445.png 480w, http://localhost:1313/2025/aiwhy-machines-learn/2_hu_64a170845a1ba395.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上图的左侧就是给神经元的输入，然后中间的g，f就代表的是神经元对于输入的处理，然后再到右侧的输入y（其实我们可以在g，f的中间再加一个传输的过程，就是将g的处理结果传输到下一个f神经元处）。&lt;/p&gt;
&lt;p&gt;然后这里假设 $x_1，x_2 \in {0,1}$,并且神经元会这样处理输入：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sum  = x1+x2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If  $Sum\geq \theta :y=1$&lt;/p&gt;
&lt;p&gt;else:y = 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以这里我们就可以认为g其实就是对输入做了一个加法，而f函数就是对g的输出做了一个判断，但是这个 $\theta$要根据具体的情况而定的（这也是人脑的神秘之处），这一整个可以表示为：&lt;/p&gt;
&lt;p&gt;$$
f(g(x)) =
\begin{cases}0, &amp;amp; g(x) &amp;lt; \theta \\
1, &amp;amp; g(x) \geq \theta
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;有了这样的前提，我们就可以来设计基础的布尔逻辑门的操作了。&lt;/p&gt;
&lt;p&gt;首先对于AND逻辑来说：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;x1&lt;/th&gt;
          &lt;th&gt;x2&lt;/th&gt;
          &lt;th&gt;sum&lt;/th&gt;
          &lt;th&gt;x1 And x2&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们可以看到这么一个逻辑对于Sum小于等于1的输出都是0，而大于1的就是1，所以我们要神经元完成这个逻辑只需要将 $\theta = 2$ ,得到：&lt;/p&gt;
&lt;p&gt;$$
f(g(x)) =
\begin{cases}0, &amp;amp; g(x) &amp;lt; 2 \\
1, &amp;amp; g(x) \geq 2
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;对于OR逻辑操作也是同理，读者可以自行一试（答案是 $\theta=1$）。&lt;/p&gt;
&lt;p&gt;但是这里有一个比较有意思的问题：但神经元需要处理不同种类的逻辑的时候，他是如何调整这个 $\theta$的值呢？&lt;/p&gt;
&lt;p&gt;Tips:下次记得分段公式要三条斜杆（调试了一上午）&lt;/p&gt;
&lt;h2 id=&#34;learning-from-mistakes&#34;&gt;Learning From Mistakes：
&lt;/h2&gt;
&lt;p&gt;我们常说，要从失败中汲取教训，这句话对于早期的Machine来说有所表现。早期Rosenblatt和Nagy 开发出的Mark I神经网络模型（就是根据前面的神经元模型建造的，这个名字让我想起Severance的Mark 🤣），这个模型能够识别图片的字母（20*20像素的图片）。但是据我们所知，光学识别系统也可以完成一样的工作，那么我们可以说它们是等价的吗？&lt;/p&gt;
&lt;p&gt;不可以，因为Mark I的工作原理与光学识别系统之间还是有所不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;光学系统识别图片上的字母&lt;/strong&gt;（OCR）就像一个经过训练的侦探。它首先“清理”图像，使其更易于分析（预处理）。然后，“划分”图像，找到哪里有文本，并将文本分解成独立的字母（分割）。接着，“观察”每个字母的特点（特征提取），最后根据这些特点与已知字母的特征进行“对比”，从而确定这个字母是哪个（分类和识别）。&lt;/p&gt;
&lt;p&gt;而Mark I识别图片中的字母则是通过学习，这个学习的过程是：每次当它错误的识别这个图像的时候，通过一定的反馈来学习如何识别图片的字母，本质上就是学习如何区分不同种类的字母。&lt;/p&gt;
&lt;p&gt;那么，具体在Perceptron里面是如何实现的呢？算法上其实和前面的神经元很像，就像下图一样：&lt;img src=&#34;http://localhost:1313/2025/aiwhy-machines-learn/4.png&#34;
	width=&#34;1444&#34;
	height=&#34;570&#34;
	srcset=&#34;http://localhost:1313/2025/aiwhy-machines-learn/4_hu_6075cf4420130cac.png 480w, http://localhost:1313/2025/aiwhy-machines-learn/4_hu_11da06f41b0cccd8.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;截屏2025-05-20 20.10.37&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;253&#34;
		data-flex-basis=&#34;608px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;它的过程是这样的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先计算 $Sum = w_1x_1+w_2x_2+b$，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后 If:  $Sum \leq 0 : y = 1$ ;&lt;/p&gt;
&lt;p&gt;​	Else:   y  =-1.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;整个过程可以简化为两个函数$g和f$，一个函数$g$负责做求和操作：
$$
g(\vec{x}) = \sum_{i=1}^{n} w_ix_i+b,\quad 其中\vec{x} = (x_1,\dots,x_n)
$$
然后函数f做判断：
$$
f(g(x)) =
\begin{cases}-1, &amp;amp; g(x) &amp;lt; 0 \\
1, &amp;amp; g(x) \geq 0
\end{cases}
$$
我们可以看到这个操作与前面有一点不一样，但都是先对于输入进行一定的处理，然后进行判断。然后我们看看这样的操作可以如何区分不同的种类的字母。先举个例子：&lt;/p&gt;
&lt;p&gt;如何判断一个人是否肥胖呢？首先，我们有一个用于训练的数据集${x_{train}=(x_1,x_2,index)}$，每个人都有特定的身高（$x_1$）和体重（$x_2$）以及是否肥胖的指标（index），例如一个人（100kg，160cm，1（是））就表示他体重100kg，身高160cm，属于肥胖人群。如果我们将瘦的人标记为三角形，胖的人标记为圆形，那么这个数据集在坐标轴就会如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/aiwhy-machines-learn/5.png&#34;
	width=&#34;767&#34;
	height=&#34;586&#34;
	srcset=&#34;http://localhost:1313/2025/aiwhy-machines-learn/5_hu_455dd33cfca4b775.png 480w, http://localhost:1313/2025/aiwhy-machines-learn/5_hu_97b80f056f3757b.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;截屏2025-05-20 20.13.09&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;130&#34;
		data-flex-basis=&#34;314px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;​      我们可以看到，Mark I在学习如何将这样的数据集分成两部分，即学习数据中的某种规律（学习这条separating line 的slope和bias），但是这样分类的学习有一定的前提：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们所提供的数据集是可以分的&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;并且这种通过training data学习到的分类模型，在面对不在training data里面的数据的时候并不能100%地做出正确的分类，这是因为，我们可以看到上图中，三角形所在的区域和圆形所在的区域之间存在很多条可以区分训练数据的直线，所以仅仅通过训练数据来训练这个分类模型是不够的，它还得用一些testing data来进行反馈（这里很像人，人在看了很久的书之后，你要让他在新的环境下去实践，才会提高人对于这本书的掌握。）&lt;/p&gt;
&lt;p&gt;所以Learning from  mistake 还是有一定部分从人类角度的启发的。并且我们可以看到上面神经元最后的输出其实可以是任意的数，那就使得这个神经元可以做的事情很多了&lt;/p&gt;
&lt;p&gt;本文原载于 &lt;a class=&#34;link&#34; href=&#34;https://quantum51.top&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;quantum51.top&lt;/a&gt;，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
